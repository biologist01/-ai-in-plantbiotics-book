---
sidebar_position: 6
---

# Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹: Ø®ØµÙˆØµÛŒØª Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ Ø³Ø³Ù¹Ù…

## Ù…Ù†ØµÙˆØ¨Û’ Ú©Ø§ Ø¬Ø§Ø¦Ø²Û

Ø§Ø³ Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹ Ù…ÛŒÚº ÛÙ… Ø§ÛŒÚ© Ù…Ú©Ù…Ù„ Ø¬ÛŒÙ†ÙˆÙ…Ú© Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ø¨Ù†Ø§Ø¦ÛŒÚº Ú¯Û’ Ø¬Ùˆ SNP ÚˆÛŒÙ¹Ø§ Ø³Û’ Ù¾ÙˆØ¯ÙˆÚº Ú©ÛŒ Ø®ØµÙˆØµÛŒØ§Øª Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ Ú©Ø±Û’ ğŸ§¬Û”

## Ù…Ù‚Ø§ØµØ¯

- SNP ÚˆÛŒÙ¹Ø§ Ù¾Ø±ÙˆØ³ÛŒØ³ Ú©Ø±Ù†Ø§
- Ø¬ÛŒÙ†ÙˆÙ…Ú© ÙÛŒÚ†Ø±Ø² Ù†Ú©Ø§Ù„Ù†Ø§
- Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ Ù…Ø§ÚˆÙ„ Ø¨Ù†Ø§Ù†Ø§
- Ù…Ø§ÚˆÙ„ Ú©ÛŒ ØªØ´Ø®ÛŒØµ

## ÚˆÛŒÙ¹Ø§ Ø³ÛŒÙ¹ ØªÛŒØ§Ø±ÛŒ

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¬ÛŒÙ†ÙˆÙ…Ú© ÚˆÛŒÙ¹Ø§
np.random.seed(42)

n_samples = 500
n_markers = 5000

# Ø¬ÛŒÙ†ÙˆÙ¹Ø§Ø¦Ù¾ Ù…ÛŒÙ¹Ø±Ú©Ø³ (0, 1, 2)
genotypes = np.random.randint(0, 3, size=(n_samples, n_markers))

# QTL Ø§Ø«Ø±Ø§Øª
n_qtl = 50
qtl_positions = np.random.choice(n_markers, n_qtl, replace=False)
qtl_effects = np.random.normal(0, 1, n_qtl)

# ÙÛŒÙ†ÙˆÙ¹Ø§Ø¦Ù¾
genetic_values = genotypes[:, qtl_positions] @ qtl_effects
noise = np.random.normal(0, 2, n_samples)
phenotypes = genetic_values + noise

print(f"Ø¬ÛŒÙ†ÙˆÙ¹Ø§Ø¦Ù¾ Ø´ÛŒÙ¾: {genotypes.shape}")
print(f"ÙÛŒÙ†ÙˆÙ¹Ø§Ø¦Ù¾ Ø±ÛŒÙ†Ø¬: {phenotypes.min():.2f} - {phenotypes.max():.2f}")
```

## ÚˆÛŒÙ¹Ø§ Ù¾Ø±ÛŒ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯

```python
def preprocess_genotypes(X):
    """
    Ø¬ÛŒÙ†ÙˆÙ¹Ø§Ø¦Ù¾ Ù¾Ø±ÛŒ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯
    """
    # Ù…Ø³Ù†Ú¯ ÙˆÛŒÙ„ÛŒÙˆØ² Ø§Ù…Ù¾ÛŒÙˆÙ¹ Ú©Ø±ÛŒÚº
    X = np.where(np.isnan(X), 1, X)
    
    # MAF ÙÙ„Ù¹Ø±
    maf = np.mean(X, axis=0) / 2
    maf = np.minimum(maf, 1 - maf)
    keep = maf > 0.05
    X = X[:, keep]
    
    # Ø³ÛŒÙ†Ù¹Ø± Ø§ÙˆØ± Ø³Ú©ÛŒÙ„
    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    X = np.nan_to_num(X, 0)
    
    return X

# Ù¾Ø±ÙˆØ³ÛŒØ³ Ú©Ø±ÛŒÚº
X_processed = preprocess_genotypes(genotypes.astype(float))
print(f"Ù¾Ø±ÙˆØ³ÛŒØ³Úˆ Ø´ÛŒÙ¾: {X_processed.shape}")

# Ù¹Ø±ÛŒÙ†/Ù¹ÛŒØ³Ù¹ ØªÙ‚Ø³ÛŒÙ…
X_train, X_test, y_train, y_test = train_test_split(
    X_processed, phenotypes, test_size=0.2, random_state=42
)
```

## Ù…Ø§ÚˆÙ„ 1: Ridge Regression

```python
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score

def train_ridge_model(X_train, y_train, X_test, y_test):
    # Ø¨ÛØªØ±ÛŒÙ† Ø§Ù„ÙØ§ ØªÙ„Ø§Ø´ Ú©Ø±ÛŒÚº
    alphas = [0.1, 1, 10, 100, 1000]
    best_alpha = None
    best_score = -np.inf
    
    for alpha in alphas:
        model = Ridge(alpha=alpha)
        model.fit(X_train, y_train)
        score = model.score(X_test, y_test)
        
        if score > best_score:
            best_score = score
            best_alpha = alpha
    
    # ÙØ§Ø¦Ù†Ù„ Ù…Ø§ÚˆÙ„
    final_model = Ridge(alpha=best_alpha)
    final_model.fit(X_train, y_train)
    
    return final_model, best_alpha

model_ridge, alpha = train_ridge_model(X_train, y_train, X_test, y_test)
y_pred_ridge = model_ridge.predict(X_test)

print(f"Ø¨ÛØªØ±ÛŒÙ† Ø§Ù„ÙØ§: {alpha}")
print(f"RÂ²: {r2_score(y_test, y_pred_ridge):.3f}")
print(f"Ø§Ø±ØªØ¨Ø§Ø·: {np.corrcoef(y_test, y_pred_ridge)[0,1]:.3f}")
```

## Ù…Ø§ÚˆÙ„ 2: Ù†ÛŒÙˆØ±Ù„ Ù†ÛŒÙ¹ÙˆØ±Ú©

```python
import tensorflow as tf
from tensorflow.keras import layers, models

def create_genomic_nn(input_dim):
    model = models.Sequential([
        layers.Dense(256, activation='relu', input_shape=(input_dim,)),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        
        layers.Dense(128, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.2),
        
        layers.Dense(1)
    ])
    
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='mse',
        metrics=['mae']
    )
    
    return model

# Ù…Ø§ÚˆÙ„ Ø¨Ù†Ø§Ø¦ÛŒÚº
model_nn = create_genomic_nn(X_train.shape[1])
model_nn.summary()

# Ú©Ø§Ù„ Ø¨ÛŒÚ©Ø³
callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
]

# Ù¹Ø±ÛŒÙ† Ú©Ø±ÛŒÚº
history = model_nn.fit(
    X_train, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    callbacks=callbacks,
    verbose=1
)
```

## Ù…Ø§ÚˆÙ„ 3: Gradient Boosting

```python
from sklearn.ensemble import GradientBoostingRegressor

# PCA Ø³Û’ ÙÛŒÚ†Ø±Ø² Ú©Ù… Ú©Ø±ÛŒÚº
from sklearn.decomposition import PCA

pca = PCA(n_components=100)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Gradient Boosting
model_gb = GradientBoostingRegressor(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42
)

model_gb.fit(X_train_pca, y_train)
y_pred_gb = model_gb.predict(X_test_pca)

print(f"Gradient Boosting RÂ²: {r2_score(y_test, y_pred_gb):.3f}")
```

## Ù…Ø§ÚˆÙ„Ø² Ú©Ø§ Ù…ÙˆØ§Ø²Ù†Û

```python
import matplotlib.pyplot as plt

def compare_models(y_test, predictions_dict):
    fig, axes = plt.subplots(1, len(predictions_dict), figsize=(15, 5))
    
    for idx, (name, y_pred) in enumerate(predictions_dict.items()):
        ax = axes[idx]
        
        corr = np.corrcoef(y_test, y_pred)[0, 1]
        r2 = r2_score(y_test, y_pred)
        
        ax.scatter(y_test, y_pred, alpha=0.5)
        ax.plot([y_test.min(), y_test.max()], 
                [y_test.min(), y_test.max()], 'r--')
        ax.set_xlabel('Ø­Ù‚ÛŒÙ‚ÛŒ Ù‚Ø¯Ø±')
        ax.set_ylabel('Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ')
        ax.set_title(f'{name}\nr={corr:.3f}, RÂ²={r2:.3f}')
    
    plt.tight_layout()
    plt.savefig('model_comparison.png', dpi=150)
    plt.show()

# Neural Network Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ
y_pred_nn = model_nn.predict(X_test).flatten()

# Ù…ÙˆØ§Ø²Ù†Û
predictions = {
    'Ridge': y_pred_ridge,
    'Neural Network': y_pred_nn,
    'Gradient Boosting': y_pred_gb
}

compare_models(y_test, predictions)
```

## ÙÛŒÚ†Ø± Ø§ÛÙ…ÛŒØª

```python
def get_top_markers(model, n_top=20):
    """
    Ø³Ø¨ Ø³Û’ Ø§ÛÙ… Ù…Ø§Ø±Ú©Ø±Ø² Ù†Ú©Ø§Ù„ÛŒÚº
    """
    coefficients = np.abs(model.coef_)
    top_indices = np.argsort(coefficients)[-n_top:]
    
    return top_indices, coefficients[top_indices]

# Ridge Ù…Ø§ÚˆÙ„ Ø³Û’
top_markers, importances = get_top_markers(model_ridge)

plt.figure(figsize=(10, 6))
plt.barh(range(len(top_markers)), importances)
plt.xlabel('Ø§ÛÙ…ÛŒØª')
plt.ylabel('Ù…Ø§Ø±Ú©Ø± Ø§Ù†ÚˆÛŒÚ©Ø³')
plt.title('Ø³Ø¨ Ø³Û’ Ø§ÛÙ… Ø¬ÛŒÙ†ÛŒÙ¹Ú© Ù…Ø§Ø±Ú©Ø±Ø²')
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=150)
plt.show()
```

## Ú©Ø±Ø§Ø³ ÙˆÛŒÙ„ÛŒÚˆÛŒØ´Ù†

```python
from sklearn.model_selection import KFold

def k_fold_validation(X, y, n_folds=5):
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
    
    results = []
    
    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):
        X_train_fold = X[train_idx]
        X_test_fold = X[test_idx]
        y_train_fold = y[train_idx]
        y_test_fold = y[test_idx]
        
        # Ridge Ù…Ø§ÚˆÙ„
        model = Ridge(alpha=100)
        model.fit(X_train_fold, y_train_fold)
        y_pred = model.predict(X_test_fold)
        
        corr = np.corrcoef(y_test_fold, y_pred)[0, 1]
        r2 = r2_score(y_test_fold, y_pred)
        
        results.append({'fold': fold + 1, 'correlation': corr, 'r2': r2})
        print(f"Fold {fold + 1}: r = {corr:.3f}, RÂ² = {r2:.3f}")
    
    results_df = pd.DataFrame(results)
    print(f"\nØ§ÙˆØ³Ø·: r = {results_df['correlation'].mean():.3f} Â± "
          f"{results_df['correlation'].std():.3f}")
    
    return results_df

results = k_fold_validation(X_processed, phenotypes)
```

## Ù…Ú©Ù…Ù„ Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù†

```python
class GenomicPredictionPipeline:
    def __init__(self):
        self.pca = None
        self.model = None
        
    def fit(self, X, y):
        # PCA
        self.pca = PCA(n_components=min(100, X.shape[1]))
        X_pca = self.pca.fit_transform(X)
        
        # Ù…Ø§ÚˆÙ„
        self.model = Ridge(alpha=100)
        self.model.fit(X_pca, y)
        
        return self
    
    def predict(self, X):
        X_pca = self.pca.transform(X)
        return self.model.predict(X_pca)
    
    def evaluate(self, X, y):
        y_pred = self.predict(X)
        return {
            'correlation': np.corrcoef(y, y_pred)[0, 1],
            'r2': r2_score(y, y_pred),
            'mse': mean_squared_error(y, y_pred)
        }

# Ø§Ø³ØªØ¹Ù…Ø§Ù„
pipeline = GenomicPredictionPipeline()
pipeline.fit(X_train, y_train)

train_metrics = pipeline.evaluate(X_train, y_train)
test_metrics = pipeline.evaluate(X_test, y_test)

print("Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ù†ØªØ§Ø¦Ø¬:", train_metrics)
print("Ù¹ÛŒØ³Ù¹ Ù†ØªØ§Ø¦Ø¬:", test_metrics)
```

## Ø®Ù„Ø§ØµÛ

Ø§Ø³ Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹ Ù…ÛŒÚº ÛÙ… Ù†Û’ Ø³ÛŒÚ©Ú¾Ø§:
- Ø¬ÛŒÙ†ÙˆÙ…Ú© ÚˆÛŒÙ¹Ø§ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯
- Ù…Ø®ØªÙ„Ù ML Ù…Ø§ÚˆÙ„Ø² Ú©Ø§ Ù…ÙˆØ§Ø²Ù†Û
- Ú©Ø±Ø§Ø³ ÙˆÛŒÙ„ÛŒÚˆÛŒØ´Ù† ØªÚ©Ù†ÛŒÚ©
- Ù…Ú©Ù…Ù„ Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† ÚˆÛŒØ²Ø§Ø¦Ù†

## Ø§Ú¯Ù„Ø§ Ù…Ø§ÚˆÛŒÙˆÙ„

[Ù…Ø§ÚˆÛŒÙˆÙ„ 4: IoT ØªØ¹Ø§Ø±Ù](/docs/module-4/iot-intro)
