---
sidebar_position: 4
---

# Ø¬ÛŒÙ†ÙˆÙ…Ú© Ø³Ù„ÛŒÚ©Ø´Ù† Ø§ÙˆØ± Ø¨Ø±ÛŒÚˆÙ†Ú¯

## ØªØ¹Ø§Ø±Ù

Ø¬ÛŒÙ†ÙˆÙ…Ú© Ø³Ù„ÛŒÚ©Ø´Ù† ÙØµÙ„ÙˆÚº Ú©ÛŒ Ø¨Ø±ÛŒÚˆÙ†Ú¯ Ù¾Ø±ÙˆÚ¯Ø±Ø§Ù…ÙˆÚº Ú©Ùˆ ØªÛŒØ² Ú©Ø± Ø±ÛÛŒ ÛÛ’Û” ML Ø³Û’ Ø¨Ø±ÛŒÚˆÙ†Ú¯ ÙˆÛŒÙ„ÛŒÙˆØ² Ú©ÛŒ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ Ø§ÙˆØ± Ø¨ÛØªØ±ÛŒÙ† Ú©Ø±Ø§Ø³Ø² ÚˆÛŒØ²Ø§Ø¦Ù† Ú©Ø±ÛŒÚº ğŸŒ¾Û”

## Ø±ÙˆØ§ÛŒØªÛŒ Ø¨Ù…Ù‚Ø§Ø¨Ù„Û Ø¬ÛŒÙ†ÙˆÙ…Ú© Ø³Ù„ÛŒÚ©Ø´Ù†

| Ø·Ø±ÛŒÙ‚Û | ÙˆÙ‚Øª | Ø¯Ø±Ø³ØªÚ¯ÛŒ |
|-------|-----|--------|
| Ø±ÙˆØ§ÛŒØªÛŒ | 10-15 Ø³Ø§Ù„ | Ú©Ù… |
| Ø¬ÛŒÙ†ÙˆÙ…Ú© | 3-5 Ø³Ø§Ù„ | Ø²ÛŒØ§Ø¯Û |

## Ø¬ÛŒÙ†ÙˆÙ…Ú© Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ Ù…Ø§ÚˆÙ„Ø²

### GBLUP (Genomic BLUP)

```python
import numpy as np
from scipy.linalg import inv

def gblup(X, y):
    """
    X: Ø¬ÛŒÙ†ÙˆÙ¹Ø§Ø¦Ù¾ Ù…ÛŒÙ¹Ø±Ú©Ø³ (n x p)
    y: ÙÛŒÙ†ÙˆÙ¹Ø§Ø¦Ù¾ ÙˆÛŒÚ©Ù¹Ø±
    """
    n, p = X.shape
    
    # Ø¬ÛŒÙ†ÙˆÙ…Ú© Ø±ÛŒÙ„ÛŒØ´Ù† Ø´Ù¾ Ù…ÛŒÙ¹Ø±Ú©Ø³
    W = X - np.mean(X, axis=0)
    G = W @ W.T / p
    
    # BLUP Ø­Ù„
    h2 = 0.5  # ÛÛŒØ±ÛŒÙ¹ÛŒØ¨ÛŒÙ„Ù¹ÛŒ
    lamb = (1 - h2) / h2
    
    V = G + lamb * np.eye(n)
    V_inv = inv(V)
    
    # Ø¨Ø±ÛŒÚˆÙ†Ú¯ ÙˆÛŒÙ„ÛŒÙˆØ²
    gebv = G @ V_inv @ y
    
    return gebv
```

### Ridge Regression (rrBLUP)

```python
from sklearn.linear_model import Ridge

def rrblup(X, y, alpha=1.0):
    """
    rrBLUP Ù…Ø§ÚˆÙ„
    """
    model = Ridge(alpha=alpha)
    model.fit(X, y)
    
    # Ù…Ø§Ø±Ú©Ø± Ø§Ø«Ø±Ø§Øª
    marker_effects = model.coef_
    
    # GEBV Ø­Ø³Ø§Ø¨ Ú©Ø±ÛŒÚº
    gebv = X @ marker_effects
    
    return gebv, marker_effects
```

## ÚˆÛŒÙ¾ Ù„Ø±Ù†Ù†Ú¯ Ø³Û’ Ø®ØµÙˆØµÛŒØª Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ

```python
import tensorflow as tf
from tensorflow.keras import layers, models

def build_genomic_predictor(n_markers):
    model = models.Sequential([
        layers.Dense(512, activation='relu', input_shape=(n_markers,)),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.2),
        
        layers.Dense(1, activation='linear')
    ])
    
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

# Ø§Ø³ØªØ¹Ù…Ø§Ù„
X_train, X_test, y_train, y_test = train_test_split(genotypes, phenotypes)

model = build_genomic_predictor(n_markers=X_train.shape[1])
model.fit(X_train, y_train, epochs=100, batch_size=32,
          validation_data=(X_test, y_test))
```

## Ù…Ù„Ù¹ÛŒ Ù¹Ø±ÛŒÙ¹ Ù…Ø§ÚˆÙ„

```python
def build_multi_trait_model(n_markers, n_traits):
    input_layer = layers.Input(shape=(n_markers,))
    
    # Ù…Ø´ØªØ±Ú©Û Ù¾Ø±ØªÛŒÚº
    x = layers.Dense(512, activation='relu')(input_layer)
    x = layers.Dropout(0.3)(x)
    x = layers.Dense(256, activation='relu')(x)
    
    # ÛØ± Ø®ØµÙˆØµÛŒØª Ú©Û’ Ù„ÛŒÛ’ Ø§Ù„Ú¯ Ø¨Ø±Ø§Ù†Ú†
    outputs = []
    for i in range(n_traits):
        branch = layers.Dense(64, activation='relu')(x)
        output = layers.Dense(1, name=f'trait_{i}')(branch)
        outputs.append(output)
    
    model = models.Model(input_layer, outputs)
    model.compile(optimizer='adam', 
                  loss='mse',
                  loss_weights=[1.0] * n_traits)
    
    return model

# Ø§Ø³ØªØ¹Ù…Ø§Ù„
model = build_multi_trait_model(n_markers=10000, n_traits=3)
# Ø®ØµÙˆØµÛŒØ§Øª: Ù¾ÛŒØ¯Ø§ÙˆØ§Ø±ØŒ Ø§ÙˆÙ†Ú†Ø§Ø¦ÛŒØŒ Ù¾Ø®ØªÚ¯ÛŒ
```

## Ø¨ÛØªØ±ÛŒÙ† Ú©Ø±Ø§Ø³ Ø³Ù„ÛŒÚ©Ø´Ù†

```python
import numpy as np
from itertools import combinations

def select_optimal_crosses(parents_gebv, n_crosses=10):
    """
    Ø¨ÛØªØ±ÛŒÙ† ÙˆØ§Ù„Ø¯ÛŒÙ† Ú©Û’ Ø¬ÙˆÚ‘Û’ Ù…Ù†ØªØ®Ø¨ Ú©Ø±ÛŒÚº
    """
    n_parents = len(parents_gebv)
    crosses = []
    
    for i, j in combinations(range(n_parents), 2):
        # Ø§ÙˆØ³Ø· GEBV
        mean_gebv = (parents_gebv[i] + parents_gebv[j]) / 2
        
        # Ø¬ÛŒÙ†ÛŒØ§ØªÛŒ ØªÙ†ÙˆØ¹ (ÙØ±Ø¶ Ú©Ø±ÛŒÚº)
        diversity = abs(parents_gebv[i] - parents_gebv[j])
        
        # Ù…Ø´ØªØ±Ú©Û Ø³Ú©ÙˆØ±
        score = mean_gebv + 0.1 * diversity
        
        crosses.append({
            'parent1': i,
            'parent2': j,
            'expected_gebv': mean_gebv,
            'score': score
        })
    
    # Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ø³Ú©ÙˆØ± ÙˆØ§Ù„Û’ Ú©Ø±Ø§Ø³Ø²
    crosses.sort(key=lambda x: x['score'], reverse=True)
    
    return crosses[:n_crosses]

# Ø§Ø³ØªØ¹Ù…Ø§Ù„
parents = np.array([2.5, 3.1, 1.8, 2.9, 3.5])
best_crosses = select_optimal_crosses(parents)
for cross in best_crosses[:3]:
    print(f"ÙˆØ§Ù„Ø¯ÛŒÙ† {cross['parent1']} x {cross['parent2']}: "
          f"Ù…ØªÙˆÙ‚Ø¹ GEBV = {cross['expected_gebv']:.2f}")
```

## Ú©Ø±Ø§Ø³ ÙˆÛŒÙ„ÛŒÚˆÛŒØ´Ù†

```python
from sklearn.model_selection import KFold

def cross_validate_genomic_model(X, y, n_folds=5):
    kf = KFold(n_splits=n_folds, shuffle=True)
    
    correlations = []
    
    for train_idx, test_idx in kf.split(X):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        
        # Ù…Ø§ÚˆÙ„ Ù¹Ø±ÛŒÙ† Ú©Ø±ÛŒÚº
        gebv, _ = rrblup(X_train, y_train)
        
        # Ù¹ÛŒØ³Ù¹ Ù¾Ø± Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ
        _, marker_effects = rrblup(X_train, y_train)
        y_pred = X_test @ marker_effects
        
        # Ø§Ø±ØªØ¨Ø§Ø·
        corr = np.corrcoef(y_test, y_pred)[0, 1]
        correlations.append(corr)
    
    return np.mean(correlations), np.std(correlations)

# Ø§Ø³ØªØ¹Ù…Ø§Ù„
acc, std = cross_validate_genomic_model(genotypes, phenotypes)
print(f"Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒ Ø¯Ø±Ø³ØªÚ¯ÛŒ: {acc:.3f} Â± {std:.3f}")
```

## Ø®Ù„Ø§ØµÛ

- Ø¬ÛŒÙ†ÙˆÙ…Ú© Ø³Ù„ÛŒÚ©Ø´Ù† Ø¨Ø±ÛŒÚˆÙ†Ú¯ Ú©Ùˆ ØªÛŒØ² Ú©Ø±ØªÛŒ ÛÛ’
- GBLUP Ø§ÙˆØ± rrBLUP Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ù…Ø§ÚˆÙ„Ø² ÛÛŒÚº
- ÚˆÛŒÙ¾ Ù„Ø±Ù†Ù†Ú¯ Ù¾ÛŒÚ†ÛŒØ¯Û Ø®ØµÙˆØµÛŒØ§Øª Ú©Û’ Ù„ÛŒÛ’

## Ø§Ú¯Ù„Û’ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª

- [Ø¬ÛŒÙ†ÙˆÙ…Ú©Ø³ Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹](/docs/module-3/genomics-project)
